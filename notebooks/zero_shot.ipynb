{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Notebook for Zero-Shot Inference with CheXzero\n",
    "This notebook walks through how to use CheXzero to perform zero-shot inference on a chest x-ray image dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from eval import evaluate, bootstrap\n",
    "from zero_shot import make, make_true_labels, run_softmax_eval\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directories and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../checkpoints/cxr-bert/checkpoint_42000.pt', '../checkpoints/cxr-bert/checkpoint_45000.pt', '../checkpoints/cxr-bert/checkpoint.pt']\n"
     ]
    }
   ],
   "source": [
    "## Define Zero Shot Labels and Templates\n",
    "\n",
    "# ----- DIRECTORIES ------ #\n",
    "# # Padchest\n",
    "# cxr_filepath: str = '/home/ec2-user/all_raw_data/padchest/images/44_cxr.h5' # filepath of chest x-ray images (.h5)\n",
    "# cxr_true_labels_path: Optional[str] = '/home/ec2-user/all_raw_data/padchest/44_cxr_labels.csv' # (optional for evaluation) if labels are provided, provide path\n",
    "\n",
    "# CheXzero test\n",
    "cxr_filepath: str = '/home/ec2-user/CHEXLOCALIZE/CheXpert/test.h5' # filepath of chest x-ray images (.h5)\n",
    "cxr_true_labels_path: Optional[str] = '/home/ec2-user/CHEXLOCALIZE/CheXpert/test_labels_view1.csv' # (optional for evaluation) if labels are provided, provide path\n",
    "\n",
    "# # CheXzero val\n",
    "# cxr_filepath: str = '/home/ec2-user/all_raw_data/chexpert/CheXpert-v1.0-small/valid/chexpert_val.h5' # filepath of chest x-ray images (.h5)\n",
    "# cxr_true_labels_path: Optional[str] = '/home/ec2-user/all_raw_data/chexpert/CheXpert-v1.0-small/valid_view1.csv' # (optional for evaluation) if labels are provided, provide path\n",
    "\n",
    "\n",
    "model_dir: str = '../checkpoints/cxr-bert' # where pretrained models are saved (.pt) \n",
    "predictions_dir: Path = Path('../predictions') # where to save predictions\n",
    "cache_dir: str = predictions_dir / \"cached\" # where to cache ensembled predictions\n",
    "\n",
    "context_length: int = 77\n",
    "\n",
    "# ------- LABELS ------  #\n",
    "# Define labels to query each image | will return a prediction for each label\n",
    "# cxr_labels: List[str] = ['Atelectasis','Cardiomegaly', \n",
    "#                                       'Consolidation', 'Edema', 'Enlarged Cardiomediastinum', 'Fracture', 'Lung Lesion',\n",
    "#                                       'Lung Opacity', 'No Finding','Pleural Effusion', 'Pleural Other', 'Pneumonia', \n",
    "#                                       'Pneumothorax', 'Support Devices']\n",
    "cxr_labels = ['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly',\n",
    "                'Lung Opacity', 'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia',\n",
    "                'Atelectasis', 'Pneumothorax', 'Pleural Effusion', 'Pleural Other',\n",
    "                'Fracture', 'Support Devices']\n",
    "# cxr_labels = [label.lower() for label in cxr_labels]\n",
    "\n",
    "# ---- TEMPLATES ----- # \n",
    "# Define set of templates | see Figure 1 for more details                        \n",
    "cxr_pair_template: Tuple[str] = (\"{}\", \"no {}\")\n",
    "\n",
    "# ----- MODEL PATHS ------ #\n",
    "# If using ensemble, collect all model paths\n",
    "model_paths = []\n",
    "for subdir, dirs, files in os.walk(model_dir):\n",
    "    for file in files:\n",
    "        full_dir = os.path.join(subdir, file)\n",
    "        model_paths.append(full_dir)\n",
    "\n",
    "n = len(model_paths)\n",
    "\n",
    "model_paths = model_paths[-3:]\n",
    "print(model_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the model on the data set using ensembled models\n",
    "def ensemble_models(\n",
    "    model_paths: List[str], \n",
    "    cxr_filepath: str, \n",
    "    cxr_labels: List[str], \n",
    "    cxr_pair_template: Tuple[str], \n",
    "    cache_dir: str = None, \n",
    "    save_name: str = None,\n",
    "    change_text_encoder: bool = False,\n",
    ") -> Tuple[List[np.ndarray], np.ndarray]: \n",
    "    \"\"\"\n",
    "    Given a list of `model_paths`, ensemble model and return\n",
    "    predictions. Caches predictions at `cache_dir` if location provided.\n",
    "\n",
    "    Returns a list of each model's predictions and the averaged\n",
    "    set of predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    predictions = []\n",
    "    model_paths = sorted(model_paths) # ensure consistency of \n",
    "    for path in model_paths: # for each model\n",
    "        model_name = Path(path).stem\n",
    "\n",
    "        # load in model and `torch.DataLoader`\n",
    "        model, loader = make(\n",
    "            model_path=path, \n",
    "            cxr_filepath=cxr_filepath, \n",
    "            change_text_encoder=change_text_encoder,\n",
    "        ) \n",
    "        \n",
    "        # path to the cached prediction\n",
    "        if cache_dir is not None:\n",
    "            if save_name is not None: \n",
    "                cache_path = Path(cache_dir) / f\"{save_name}_{model_name}.npy\"\n",
    "            else: \n",
    "                cache_path = Path(cache_dir) / f\"{model_name}.npy\"\n",
    "\n",
    "        # if prediction already cached, don't recompute prediction\n",
    "        if cache_dir is not None and os.path.exists(cache_path): \n",
    "            print(\"Loading cached prediction for {}\".format(model_name))\n",
    "            y_pred = np.load(cache_path)\n",
    "        else: # cached prediction not found, compute preds\n",
    "            print(\"Inferring model {}\".format(path))\n",
    "            y_pred = run_softmax_eval(model, loader, cxr_labels, cxr_pair_template)\n",
    "            if cache_dir is not None: \n",
    "                Path(cache_dir).mkdir(exist_ok=True, parents=True)\n",
    "                np.save(file=cache_path, arr=y_pred)\n",
    "        predictions.append(y_pred)\n",
    "    \n",
    "    # compute average predictions\n",
    "    y_pred_avg = np.mean(predictions, axis=0)\n",
    "    \n",
    "    return predictions, y_pred_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/cs197proj/lib/python3.9/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../checkpoints/cxr-bert/checkpoint.pt using an online model  \n",
      "Argument error. Set pretrained = True. <class 'RuntimeError'>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for CLIP:\n\tUnexpected key(s) in state_dict: \"encode_text.bert.embeddings.position_ids\", \"encode_text.bert.embeddings.word_embeddings.weight\", \"encode_text.bert.embeddings.position_embeddings.weight\", \"encode_text.bert.embeddings.token_type_embeddings.weight\", \"encode_text.bert.embeddings.LayerNorm.weight\", \"encode_text.bert.embeddings.LayerNorm.bias\", \"encode_text.bert.encoder.layer.0.attention.self.query.weight\", \"encode_text.bert.encoder.layer.0.attention.self.query.bias\", \"encode_text.bert.encoder.layer.0.attention.self.key.weight\", \"encode_text.bert.encoder.layer.0.attention.self.key.bias\", \"encode_text.bert.encoder.layer.0.attention.self.value.weight\", \"encode_text.bert.encoder.layer.0.attention.self.value.bias\", \"encode_text.bert.encoder.layer.0.attention.output.dense.weight\", \"encode_text.bert.encoder.layer.0.attention.output.dense.bias\", \"encode_text.bert.encoder.layer.0.attention.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.0.attention.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.0.intermediate.dense.weight\", \"encode_text.bert.encoder.layer.0.intermediate.dense.bias\", \"encode_text.bert.encoder.layer.0.output.dense.weight\", \"encode_text.bert.encoder.layer.0.output.dense.bias\", \"encode_text.bert.encoder.layer.0.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.0.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.1.attention.self.query.weight\", \"encode_text.bert.encoder.layer.1.attention.self.query.bias\", \"encode_text.bert.encoder.layer.1.attention.self.key.weight\", \"encode_text.bert.encoder.layer.1.attention.self.key.bias\", \"encode_text.bert.encoder.layer.1.attention.self.value.weight\", \"encode_text.bert.encoder.layer.1.attention.self.value.bias\", \"encode_text.bert.encoder.layer.1.attention.output.dense.weight\", \"encode_text.bert.encoder.layer.1.attention.output.dense.bias\", \"encode_text.bert.encoder.layer.1.attention.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.1.attention.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.1.intermediate.dense.weight\", \"encode_text.bert.encoder.layer.1.intermediate.dense.bias\", \"encode_text.bert.encoder.layer.1.output.dense.weight\", \"encode_text.bert.encoder.layer.1.output.dense.bias\", \"encode_text.bert.encoder.layer.1.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.1.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.2.attention.self.query.weight\", \"encode_text.bert.encoder.layer.2.attention.self.query.bias\", \"encode_text.bert.encoder.layer.2.attention.self.key.weight\", \"encode_text.bert.encoder.layer.2.attention.self.key.bias\", \"encode_text.bert.encoder.layer.2.attention.self.value.weight\", \"encode_text.bert.encoder.layer.2.attention.self.value.bias\", \"encode_text.bert.encoder.layer.2.attention.output.dense.weight\", \"encode_text.bert.encoder.layer.2.attention.output.dense.bias\", \"encode_text.bert.encoder.layer.2.attention.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.2.attention.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.2.intermediate.dense.weight\", \"encode_text.bert.encoder.layer.2.intermediate.dense.bias\", \"encode_text.bert.encoder.layer.2.output.dense.weight\", \"encode_text.bert.encoder.layer.2.output.dense.bias\", \"encode_text.bert.encoder.layer.2.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.2.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.3.attention.self.query.weight\", \"encode_text.bert.encoder.layer.3.attention.self.query.bias\", \"encode_text.bert.encoder.layer.3.attention.self.key.weight\", \"encode_text.bert.encoder.layer.3.attention.self.key.bias\", \"encode_text.bert.encoder.layer.3.attention.self.value.weight\", \"encode_text.bert.encoder.layer.3.attention.self.value.bias\", \"encode_text.bert.encoder.layer.3.attention.output.dense.weight\", \"encode_text.bert.encoder.layer.3.attention.output.dense.bias\", \"encode_text.bert.encoder.layer.3.attention.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.3.attention.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.3.intermediate.dense.weight\", \"encode_text.bert.encoder.layer.3.intermediate.dense.bias\", \"encode_text.bert.encoder.layer.3.output.dense.weight\", \"encode_text.bert.encoder.layer.3.output.dense.bias\", \"encode_text.bert.encoder.layer.3.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.3.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.4.attention.self.query.weight\", \"encode_text.bert.encoder.layer.4.attention.self.query.bias\", \"encode_text.bert.encoder.layer.4.attention.self.key.weight\", \"encode_text.bert.encoder.layer.4.attention.self.key.bias\", \"encode_text.bert.encoder.layer.4.attention.self.value.weight\", \"encode_text.bert.encoder.layer.4.attention.self.value.bias\", \"encode_text.bert.encoder.layer.4.attention.output.dense.weight\", \"encode_text.bert.encoder.layer.4.attention.output.dense.bias\", \"encode_text.bert.encoder.layer.4.attention.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.4.attention.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.4.intermediate.dense.weight\", \"encode_text.bert.encoder.layer.4.intermediate.dense.bias\", \"encode_text.bert.encoder.layer.4.output.dense.weight\", \"encode_text.bert.encoder.layer.4.output.dense.bias\", \"encode_text.bert.encoder.layer.4.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.4.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.5.attention.self.query.weight\", \"encode_text.bert.encoder.layer.5.attention.self.query.bias\", \"encode_text.bert.encoder.layer.5.attention.self.key.weight\", \"encode_text.bert.encoder.layer.5.attention.self.key.bias\", \"encode_text.bert.encoder.layer.5.attention.self.value.weight\", \"encode_text.bert.encoder.layer.5.attention.self.value.bias\", \"encode_text.bert.encoder.layer.5.attention.output.dense.weight\", \"encode_text.bert.encoder.layer.5.attention.output.dense.bias\", \"encode_text.bert.encoder.layer.5.attention.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.5.attention.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.5.intermediate.dense.weight\", \"encode_text.bert.encoder.layer.5.intermediate.dense.bias\", \"encode_text.bert.encoder.layer.5.output.dense.weight\", \"encode_text.bert.encoder.layer.5.output.dense.bias\", \"encode_text.bert.encoder.layer.5.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.5.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.6.attention.self.query.weight\", \"encode_text.bert.encoder.layer.6.attention.self.query.bias\", \"encode_text.bert.encoder.layer.6.attention.self.key.weight\", \"encode_text.bert.encoder.layer.6.attention.self.key.bias\", \"encode_text.bert.encoder.layer.6.attention.self.value.weight\", \"encode_text.bert.encoder.layer.6.attention.self.value.bias\", \"encode_text.bert.encoder.layer.6.attention.output.dense.weight\", \"encode_text.bert.encoder.layer.6.attention.output.dense.bias\", \"encode_text.bert.encoder.layer.6.attention.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.6.attention.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.6.intermediate.dense.weight\", \"encode_text.bert.encoder.layer.6.intermediate.dense.bias\", \"encode_text.bert.encoder.layer.6.output.dense.weight\", \"encode_text.bert.encoder.layer.6.output.dense.bias\", \"encode_text.bert.encoder.layer.6.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.6.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.7.attention.self.query.weight\", \"encode_text.bert.encoder.layer.7.attention.self.query.bias\", \"encode_text.bert.encoder.layer.7.attention.self.key.weight\", \"encode_text.bert.encoder.layer.7.attention.self.key.bias\", \"encode_text.bert.encoder.layer.7.attention.self.value.weight\", \"encode_text.bert.encoder.layer.7.attention.self.value.bias\", \"encode_text.bert.encoder.layer.7.attention.output.dense.weight\", \"encode_text.bert.encoder.layer.7.attention.output.dense.bias\", \"encode_text.bert.encoder.layer.7.attention.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.7.attention.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.7.intermediate.dense.weight\", \"encode_text.bert.encoder.layer.7.intermediate.dense.bias\", \"encode_text.bert.encoder.layer.7.output.dense.weight\", \"encode_text.bert.encoder.layer.7.output.dense.bias\", \"encode_text.bert.encoder.layer.7.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.7.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.8.attention.self.query.weight\", \"encode_text.bert.encoder.layer.8.attention.self.query.bias\", \"encode_text.bert.encoder.layer.8.attention.self.key.weight\", \"encode_text.bert.encoder.layer.8.attention.self.key.bias\", \"encode_text.bert.encoder.layer.8.attention.self.value.weight\", \"encode_text.bert.encoder.layer.8.attention.self.value.bias\", \"encode_text.bert.encoder.layer.8.attention.output.dense.weight\", \"encode_text.bert.encoder.layer.8.attention.output.dense.bias\", \"encode_text.bert.encoder.layer.8.attention.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.8.attention.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.8.intermediate.dense.weight\", \"encode_text.bert.encoder.layer.8.intermediate.dense.bias\", \"encode_text.bert.encoder.layer.8.output.dense.weight\", \"encode_text.bert.encoder.layer.8.output.dense.bias\", \"encode_text.bert.encoder.layer.8.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.8.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.9.attention.self.query.weight\", \"encode_text.bert.encoder.layer.9.attention.self.query.bias\", \"encode_text.bert.encoder.layer.9.attention.self.key.weight\", \"encode_text.bert.encoder.layer.9.attention.self.key.bias\", \"encode_text.bert.encoder.layer.9.attention.self.value.weight\", \"encode_text.bert.encoder.layer.9.attention.self.value.bias\", \"encode_text.bert.encoder.layer.9.attention.output.dense.weight\", \"encode_text.bert.encoder.layer.9.attention.output.dense.bias\", \"encode_text.bert.encoder.layer.9.attention.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.9.attention.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.9.intermediate.dense.weight\", \"encode_text.bert.encoder.layer.9.intermediate.dense.bias\", \"encode_text.bert.encoder.layer.9.output.dense.weight\", \"encode_text.bert.encoder.layer.9.output.dense.bias\", \"encode_text.bert.encoder.layer.9.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.9.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.10.attention.self.query.weight\", \"encode_text.bert.encoder.layer.10.attention.self.query.bias\", \"encode_text.bert.encoder.layer.10.attention.self.key.weight\", \"encode_text.bert.encoder.layer.10.attention.self.key.bias\", \"encode_text.bert.encoder.layer.10.attention.self.value.weight\", \"encode_text.bert.encoder.layer.10.attention.self.value.bias\", \"encode_text.bert.encoder.layer.10.attention.output.dense.weight\", \"encode_text.bert.encoder.layer.10.attention.output.dense.bias\", \"encode_text.bert.encoder.layer.10.attention.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.10.attention.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.10.intermediate.dense.weight\", \"encode_text.bert.encoder.layer.10.intermediate.dense.bias\", \"encode_text.bert.encoder.layer.10.output.dense.weight\", \"encode_text.bert.encoder.layer.10.output.dense.bias\", \"encode_text.bert.encoder.layer.10.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.10.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.11.attention.self.query.weight\", \"encode_text.bert.encoder.layer.11.attention.self.query.bias\", \"encode_text.bert.encoder.layer.11.attention.self.key.weight\", \"encode_text.bert.encoder.layer.11.attention.self.key.bias\", \"encode_text.bert.encoder.layer.11.attention.self.value.weight\", \"encode_text.bert.encoder.layer.11.attention.self.value.bias\", \"encode_text.bert.encoder.layer.11.attention.output.dense.weight\", \"encode_text.bert.encoder.layer.11.attention.output.dense.bias\", \"encode_text.bert.encoder.layer.11.attention.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.11.attention.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.11.intermediate.dense.weight\", \"encode_text.bert.encoder.layer.11.intermediate.dense.bias\", \"encode_text.bert.encoder.layer.11.output.dense.weight\", \"encode_text.bert.encoder.layer.11.output.dense.bias\", \"encode_text.bert.encoder.layer.11.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.11.output.LayerNorm.bias\", \"encode_text.cls.predictions.bias\", \"encode_text.cls.predictions.transform.dense.weight\", \"encode_text.cls.predictions.transform.dense.bias\", \"encode_text.cls.predictions.transform.LayerNorm.weight\", \"encode_text.cls.predictions.transform.LayerNorm.bias\", \"encode_text.cls.predictions.decoder.weight\", \"encode_text.cls.predictions.decoder.bias\", \"encode_text.cls_projection_head.dense_to_hidden.weight\", \"encode_text.cls_projection_head.dense_to_hidden.bias\", \"encode_text.cls_projection_head.LayerNorm.weight\", \"encode_text.cls_projection_head.LayerNorm.bias\", \"encode_text.cls_projection_head.dense_to_output.weight\", \"encode_text.cls_projection_head.dense_to_output.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predictions, y_pred_avg \u001b[39m=\u001b[39m ensemble_models(\n\u001b[1;32m      2\u001b[0m     model_paths\u001b[39m=\u001b[39;49mmodel_paths, \n\u001b[1;32m      3\u001b[0m     cxr_filepath\u001b[39m=\u001b[39;49mcxr_filepath, \n\u001b[1;32m      4\u001b[0m     cxr_labels\u001b[39m=\u001b[39;49mcxr_labels, \n\u001b[1;32m      5\u001b[0m     cxr_pair_template\u001b[39m=\u001b[39;49mcxr_pair_template, \n\u001b[1;32m      6\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m      7\u001b[0m     change_text_encoder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, \u001b[39m# don't use CXR BERT\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m )\n",
      "Cell \u001b[0;32mIn [3], line 25\u001b[0m, in \u001b[0;36mensemble_models\u001b[0;34m(model_paths, cxr_filepath, cxr_labels, cxr_pair_template, cache_dir, save_name, change_text_encoder)\u001b[0m\n\u001b[1;32m     22\u001b[0m model_name \u001b[39m=\u001b[39m Path(path)\u001b[39m.\u001b[39mstem\n\u001b[1;32m     24\u001b[0m \u001b[39m# load in model and `torch.DataLoader`\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m model, loader \u001b[39m=\u001b[39m make(\n\u001b[1;32m     26\u001b[0m     model_path\u001b[39m=\u001b[39;49mpath, \n\u001b[1;32m     27\u001b[0m     cxr_filepath\u001b[39m=\u001b[39;49mcxr_filepath, \n\u001b[1;32m     28\u001b[0m     change_text_encoder\u001b[39m=\u001b[39;49mchange_text_encoder,\n\u001b[1;32m     29\u001b[0m ) \n\u001b[1;32m     31\u001b[0m \u001b[39m# path to the cached prediction\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39mif\u001b[39;00m cache_dir \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/CheXzero/notebooks/../zero_shot.py:386\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(model_path, cxr_filepath, pretrained, context_length, change_text_encoder)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[39mFUNCTION: make\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[39m-------------------------------------------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[39mReturns model, data loader. \u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[39m# load model\u001b[39;00m\n\u001b[0;32m--> 386\u001b[0m model \u001b[39m=\u001b[39m load_clip(\n\u001b[1;32m    387\u001b[0m     model_path\u001b[39m=\u001b[39;49mmodel_path, \n\u001b[1;32m    388\u001b[0m     pretrained\u001b[39m=\u001b[39;49mpretrained, \n\u001b[1;32m    389\u001b[0m     context_length\u001b[39m=\u001b[39;49mcontext_length,\n\u001b[1;32m    390\u001b[0m     change_text_encoder\u001b[39m=\u001b[39;49mchange_text_encoder,\n\u001b[1;32m    391\u001b[0m )\n\u001b[1;32m    393\u001b[0m \u001b[39m# load data\u001b[39;00m\n\u001b[1;32m    394\u001b[0m transformations \u001b[39m=\u001b[39m [\n\u001b[1;32m    395\u001b[0m     \u001b[39m# means computed from sample in `cxr_stats` notebook\u001b[39;00m\n\u001b[1;32m    396\u001b[0m     Normalize((\u001b[39m101.48761\u001b[39m, \u001b[39m101.48761\u001b[39m, \u001b[39m101.48761\u001b[39m), (\u001b[39m83.43944\u001b[39m, \u001b[39m83.43944\u001b[39m, \u001b[39m83.43944\u001b[39m)),\n\u001b[1;32m    397\u001b[0m ]\n",
      "File \u001b[0;32m~/CheXzero/notebooks/../zero_shot.py:99\u001b[0m, in \u001b[0;36mload_clip\u001b[0;34m(model_path, pretrained, context_length, change_text_encoder)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[39mprint\u001b[39m(model_path,\u001b[39m\"\u001b[39m\u001b[39musing an online model  \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     98\u001b[0m     \u001b[39m# model=torch.load(model_path).to(device)\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m     model\u001b[39m.\u001b[39;49mload_state_dict(torch\u001b[39m.\u001b[39;49mload(model_path))\n\u001b[1;32m    100\u001b[0m     model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    101\u001b[0m \u001b[39mexcept\u001b[39;00m: \n",
      "File \u001b[0;32m/opt/conda/envs/cs197proj/lib/python3.9/site-packages/torch/nn/modules/module.py:1482\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1477\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   1478\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1479\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1481\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 1482\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1483\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1484\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CLIP:\n\tUnexpected key(s) in state_dict: \"encode_text.bert.embeddings.position_ids\", \"encode_text.bert.embeddings.word_embeddings.weight\", \"encode_text.bert.embeddings.position_embeddings.weight\", \"encode_text.bert.embeddings.token_type_embeddings.weight\", \"encode_text.bert.embeddings.LayerNorm.weight\", \"encode_text.bert.embeddings.LayerNorm.bias\", \"encode_text.bert.encoder.layer.0.attention.self.query.weight\", \"encode_text.bert.encoder.layer.0.attention.self.query.bias\", \"encode_text.bert.encoder.layer.0.attention.self.key.weight\", \"encode_text.bert.encoder.layer.0.attention.self.key.bias\", \"encode_text.bert.encoder.layer.0.attention.self.value.weight\", \"encode_text.bert.encoder.layer.0.attention.self.value.bias\", \"encode_text.bert.encoder.layer.0.attention.output.dense.weight\", \"encode_text.bert.encoder.layer.0.attention.output.dense.bias\", \"encode_text.bert.encoder.layer.0.attention.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.0.attention.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.0.intermediate.dense.weight\", \"encode_text.bert.encoder.layer.0.intermediate.dense.bias\", \"encode_text.bert.encoder.layer.0.output.dense.weight\", \"encode_text.bert.encoder.layer.0.output.dense.bias\", \"encode_text.bert.encoder.layer.0.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.0.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.1.attention.self.query.weight\", \"encode_text.bert.encoder.layer.1.attention.self.query.bias\", \"encode_text.bert.encoder.layer.1.attention.self.key.weight\", \"encode_text.bert.encoder.layer.1.attention.self.key.bias\", \"encode_text.bert.encoder.layer.1.attention.self.value.weight\", \"encode_text.bert.encoder.layer.1.attention.self.value.bias\", \"encode_text.bert.encoder.layer.1.attention.output.dense.weight\", \"encode_text.bert.encoder.layer.1.attention.output.dense.bias\", \"encode_text.bert.encoder.layer.1.attention.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.1.attention.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.1.intermediate.dense.weight\", \"encode_text.bert.encoder.layer.1.intermediate.dense.bias\", \"encode_text.bert.encoder.layer.1.output.dense.weight\", \"encode_text.bert.encoder.layer.1.output.dense.bias\", \"encode_text.bert.encoder.layer.1.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.1.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.2.attention.self.query.weight\", \"encode_text.bert.encoder.layer.2.attention.self.query.bias\", \"encode_text.bert.encoder.layer.2.attention.self.key.weight\", \"encode_text.bert.encoder.layer.2.attention.self.key.bias\", \"encode_text.bert.encoder.layer.2.attention.self.value.weight\", \"encode_text.bert.encoder.layer.2.attention.self.value.bias\", \"encode_text.bert.encoder.layer.2.attention.output.dense.weight\", \"encode_text.bert.encoder.layer.2.attention.output.dense.bias\", \"encode_text.bert.encoder.layer.2.attention.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.2.attention.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.2.intermediate.dense.weight\", \"encode_text.bert.encoder.layer.2.intermediate.dense.bias\", \"encode_text.bert.encoder.layer.2.output.dense.weight\", \"encode_text.bert.encoder.layer.2.output.dense.bias\", \"encode_text.bert.encoder.layer.2.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.2.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.3.attention.self.query.weight\", \"encode_text.bert.encoder.layer.3.attention.self.query.bias\", \"encode_text.bert.encoder.layer.3.attention.self.key.weight\", \"encode_text.bert.encoder.layer.3.attention.self.key.bias\", \"encode_text.bert.encoder.layer.3.attention.self.value.weight\", \"encode_text.bert.encoder.layer.3.attention.self.value.bias\", \"encode_text.bert.encoder.layer.3.attention.output.dense.weight\", \"encode_text.bert.encoder.layer.3.attention.output.dense.bias\", \"encode_text.bert.encoder.layer.3.attention.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.3.attention.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.3.intermediate.dense.weight\", \"encode_text.bert.encoder.layer.3.intermediate.dense.bias\", \"encode_text.bert.encoder.layer.3.output.dense.weight\", \"encode_text.bert.encoder.layer.3.output.dense.bias\", \"encode_text.bert.encoder.layer.3.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.3.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.4.attention.self.query.weight\", \"encode_text.bert.encoder.layer.4.attention.self.query.bias\", \"encode_text.bert.encoder.layer.4.attention.self.key.weight\", \"encode_text.bert.encoder.layer.4.attention.self.key.bias\", \"encode_text.bert.encoder.layer.4.attention.self.value.weight\", \"encode_text.bert.encoder.layer.4.attention.self.value.bias\", \"encode_text.bert.encoder.layer.4.attention.output.dense.weight\", \"encode_text.bert.encoder.layer.4.attention.output.dense.bias\", \"encode_text.bert.encoder.layer.4.attention.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.4.attention.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.4.intermediate.dense.weight\", \"encode_text.bert.encoder.layer.4.intermediate.dense.bias\", \"encode_text.bert.encoder.layer.4.output.dense.weight\", \"encode_text.bert.encoder.layer.4.output.dense.bias\", \"encode_text.bert.encoder.layer.4.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.4.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.5.attention.self.query.weight\", \"encode_text.bert.encoder.layer.5.attention.self.query.bias\", \"encode_text.bert.encoder.layer.5.attention.self.key.weight\", \"encode_text.bert.encoder.layer.5.attention.self.key.bias\", \"encode_text.bert.encoder.layer.5.attention.self.value.weight\", \"encode_text.bert.encoder.layer.5.attention.self.value.bias\", \"encode_text.bert.encoder.layer.5.attention.output.dense.weight\", \"encode_text.bert.encoder.layer.5.attention.output.dense.bias\", \"encode_text.bert.encoder.layer.5.attention.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.5.attention.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.5.intermediate.dense.weight\", \"encode_text.bert.encoder.layer.5.intermediate.dense.bias\", \"encode_text.bert.encoder.layer.5.output.dense.weight\", \"encode_text.bert.encoder.layer.5.output.dense.bias\", \"encode_text.bert.encoder.layer.5.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.5.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.6.attention.self.query.weight\", \"encode_text.bert.encoder.layer.6.attention.self.query.bias\", \"encode_text.bert.encoder.layer.6.attention.self.key.weight\", \"encode_text.bert.encoder.layer.6.attention.self.key.bias\", \"encode_text.bert.encoder.layer.6.attention.self.value.weight\", \"encode_text.bert.encoder.layer.6.attention.self.value.bias\", \"encode_text.bert.encoder.layer.6.attention.output.dense.weight\", \"encode_text.bert.encoder.layer.6.attention.output.dense.bias\", \"encode_text.bert.encoder.layer.6.attention.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.6.attention.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.6.intermediate.dense.weight\", \"encode_text.bert.encoder.layer.6.intermediate.dense.bias\", \"encode_text.bert.encoder.layer.6.output.dense.weight\", \"encode_text.bert.encoder.layer.6.output.dense.bias\", \"encode_text.bert.encoder.layer.6.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.6.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.7.attention.self.query.weight\", \"encode_text.bert.encoder.layer.7.attention.self.query.bias\", \"encode_text.bert.encoder.layer.7.attention.self.key.weight\", \"encode_text.bert.encoder.layer.7.attention.self.key.bias\", \"encode_text.bert.encoder.layer.7.attention.self.value.weight\", \"encode_text.bert.encoder.layer.7.attention.self.value.bias\", \"encode_text.bert.encoder.layer.7.attention.output.dense.weight\", \"encode_text.bert.encoder.layer.7.attention.output.dense.bias\", \"encode_text.bert.encoder.layer.7.attention.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.7.attention.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.7.intermediate.dense.weight\", \"encode_text.bert.encoder.layer.7.intermediate.dense.bias\", \"encode_text.bert.encoder.layer.7.output.dense.weight\", \"encode_text.bert.encoder.layer.7.output.dense.bias\", \"encode_text.bert.encoder.layer.7.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.7.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.8.attention.self.query.weight\", \"encode_text.bert.encoder.layer.8.attention.self.query.bias\", \"encode_text.bert.encoder.layer.8.attention.self.key.weight\", \"encode_text.bert.encoder.layer.8.attention.self.key.bias\", \"encode_text.bert.encoder.layer.8.attention.self.value.weight\", \"encode_text.bert.encoder.layer.8.attention.self.value.bias\", \"encode_text.bert.encoder.layer.8.attention.output.dense.weight\", \"encode_text.bert.encoder.layer.8.attention.output.dense.bias\", \"encode_text.bert.encoder.layer.8.attention.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.8.attention.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.8.intermediate.dense.weight\", \"encode_text.bert.encoder.layer.8.intermediate.dense.bias\", \"encode_text.bert.encoder.layer.8.output.dense.weight\", \"encode_text.bert.encoder.layer.8.output.dense.bias\", \"encode_text.bert.encoder.layer.8.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.8.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.9.attention.self.query.weight\", \"encode_text.bert.encoder.layer.9.attention.self.query.bias\", \"encode_text.bert.encoder.layer.9.attention.self.key.weight\", \"encode_text.bert.encoder.layer.9.attention.self.key.bias\", \"encode_text.bert.encoder.layer.9.attention.self.value.weight\", \"encode_text.bert.encoder.layer.9.attention.self.value.bias\", \"encode_text.bert.encoder.layer.9.attention.output.dense.weight\", \"encode_text.bert.encoder.layer.9.attention.output.dense.bias\", \"encode_text.bert.encoder.layer.9.attention.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.9.attention.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.9.intermediate.dense.weight\", \"encode_text.bert.encoder.layer.9.intermediate.dense.bias\", \"encode_text.bert.encoder.layer.9.output.dense.weight\", \"encode_text.bert.encoder.layer.9.output.dense.bias\", \"encode_text.bert.encoder.layer.9.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.9.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.10.attention.self.query.weight\", \"encode_text.bert.encoder.layer.10.attention.self.query.bias\", \"encode_text.bert.encoder.layer.10.attention.self.key.weight\", \"encode_text.bert.encoder.layer.10.attention.self.key.bias\", \"encode_text.bert.encoder.layer.10.attention.self.value.weight\", \"encode_text.bert.encoder.layer.10.attention.self.value.bias\", \"encode_text.bert.encoder.layer.10.attention.output.dense.weight\", \"encode_text.bert.encoder.layer.10.attention.output.dense.bias\", \"encode_text.bert.encoder.layer.10.attention.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.10.attention.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.10.intermediate.dense.weight\", \"encode_text.bert.encoder.layer.10.intermediate.dense.bias\", \"encode_text.bert.encoder.layer.10.output.dense.weight\", \"encode_text.bert.encoder.layer.10.output.dense.bias\", \"encode_text.bert.encoder.layer.10.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.10.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.11.attention.self.query.weight\", \"encode_text.bert.encoder.layer.11.attention.self.query.bias\", \"encode_text.bert.encoder.layer.11.attention.self.key.weight\", \"encode_text.bert.encoder.layer.11.attention.self.key.bias\", \"encode_text.bert.encoder.layer.11.attention.self.value.weight\", \"encode_text.bert.encoder.layer.11.attention.self.value.bias\", \"encode_text.bert.encoder.layer.11.attention.output.dense.weight\", \"encode_text.bert.encoder.layer.11.attention.output.dense.bias\", \"encode_text.bert.encoder.layer.11.attention.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.11.attention.output.LayerNorm.bias\", \"encode_text.bert.encoder.layer.11.intermediate.dense.weight\", \"encode_text.bert.encoder.layer.11.intermediate.dense.bias\", \"encode_text.bert.encoder.layer.11.output.dense.weight\", \"encode_text.bert.encoder.layer.11.output.dense.bias\", \"encode_text.bert.encoder.layer.11.output.LayerNorm.weight\", \"encode_text.bert.encoder.layer.11.output.LayerNorm.bias\", \"encode_text.cls.predictions.bias\", \"encode_text.cls.predictions.transform.dense.weight\", \"encode_text.cls.predictions.transform.dense.bias\", \"encode_text.cls.predictions.transform.LayerNorm.weight\", \"encode_text.cls.predictions.transform.LayerNorm.bias\", \"encode_text.cls.predictions.decoder.weight\", \"encode_text.cls.predictions.decoder.bias\", \"encode_text.cls_projection_head.dense_to_hidden.weight\", \"encode_text.cls_projection_head.dense_to_hidden.bias\", \"encode_text.cls_projection_head.LayerNorm.weight\", \"encode_text.cls_projection_head.LayerNorm.bias\", \"encode_text.cls_projection_head.dense_to_output.weight\", \"encode_text.cls_projection_head.dense_to_output.bias\". "
     ]
    }
   ],
   "source": [
    "predictions, y_pred_avg = ensemble_models(\n",
    "    model_paths=model_paths, \n",
    "    cxr_filepath=cxr_filepath, \n",
    "    cxr_labels=cxr_labels, \n",
    "    cxr_pair_template=cxr_pair_template, \n",
    "    cache_dir=cache_dir,\n",
    "    change_text_encoder=True, # don't use CXR BERT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save averaged preds\n",
    "pred_name = \"chexpert_preds_bert.npy\" # add name of preds\n",
    "predictions_dir = predictions_dir / pred_name\n",
    "np.save(file=predictions_dir, arr=y_pred_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Evaluate Results\n",
    "If ground truth labels are available, compute AUC on each pathology to evaluate the performance of the zero-shot model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e10cb7a37374d229138c51a18a05789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make test_true\n",
    "test_pred = y_pred_avg\n",
    "test_true = make_true_labels(cxr_true_labels_path=cxr_true_labels_path, cxr_labels=cxr_labels)\n",
    "\n",
    "# evaluate model\n",
    "cxr_results = evaluate(test_pred, test_true, cxr_labels)\n",
    "\n",
    "# boostrap evaluations for 95% confidence intervals\n",
    "bootstrap_results = bootstrap(test_pred, test_true, cxr_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No Finding_auc</th>\n",
       "      <th>Enlarged Cardiomediastinum_auc</th>\n",
       "      <th>Cardiomegaly_auc</th>\n",
       "      <th>Lung Opacity_auc</th>\n",
       "      <th>Lung Lesion_auc</th>\n",
       "      <th>Edema_auc</th>\n",
       "      <th>Consolidation_auc</th>\n",
       "      <th>Pneumonia_auc</th>\n",
       "      <th>Atelectasis_auc</th>\n",
       "      <th>Pneumothorax_auc</th>\n",
       "      <th>Pleural Effusion_auc</th>\n",
       "      <th>Pleural Other_auc</th>\n",
       "      <th>Fracture_auc</th>\n",
       "      <th>Support Devices_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.1951</td>\n",
       "      <td>0.8855</td>\n",
       "      <td>0.9121</td>\n",
       "      <td>0.9202</td>\n",
       "      <td>0.7636</td>\n",
       "      <td>0.8971</td>\n",
       "      <td>0.8793</td>\n",
       "      <td>0.7792</td>\n",
       "      <td>0.7744</td>\n",
       "      <td>0.6676</td>\n",
       "      <td>0.9284</td>\n",
       "      <td>0.5975</td>\n",
       "      <td>0.4756</td>\n",
       "      <td>0.7099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lower</th>\n",
       "      <td>0.1369</td>\n",
       "      <td>0.8564</td>\n",
       "      <td>0.8823</td>\n",
       "      <td>0.8945</td>\n",
       "      <td>0.6056</td>\n",
       "      <td>0.8640</td>\n",
       "      <td>0.8032</td>\n",
       "      <td>0.5734</td>\n",
       "      <td>0.7315</td>\n",
       "      <td>0.5041</td>\n",
       "      <td>0.9012</td>\n",
       "      <td>0.4356</td>\n",
       "      <td>0.2319</td>\n",
       "      <td>0.6621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>upper</th>\n",
       "      <td>0.2594</td>\n",
       "      <td>0.9124</td>\n",
       "      <td>0.9355</td>\n",
       "      <td>0.9415</td>\n",
       "      <td>0.9133</td>\n",
       "      <td>0.9257</td>\n",
       "      <td>0.9425</td>\n",
       "      <td>0.9528</td>\n",
       "      <td>0.8120</td>\n",
       "      <td>0.8394</td>\n",
       "      <td>0.9505</td>\n",
       "      <td>0.8438</td>\n",
       "      <td>0.7944</td>\n",
       "      <td>0.7530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       No Finding_auc  Enlarged Cardiomediastinum_auc  Cardiomegaly_auc  \\\n",
       "mean           0.1951                          0.8855            0.9121   \n",
       "lower          0.1369                          0.8564            0.8823   \n",
       "upper          0.2594                          0.9124            0.9355   \n",
       "\n",
       "       Lung Opacity_auc  Lung Lesion_auc  Edema_auc  Consolidation_auc  \\\n",
       "mean             0.9202           0.7636     0.8971             0.8793   \n",
       "lower            0.8945           0.6056     0.8640             0.8032   \n",
       "upper            0.9415           0.9133     0.9257             0.9425   \n",
       "\n",
       "       Pneumonia_auc  Atelectasis_auc  Pneumothorax_auc  Pleural Effusion_auc  \\\n",
       "mean          0.7792           0.7744            0.6676                0.9284   \n",
       "lower         0.5734           0.7315            0.5041                0.9012   \n",
       "upper         0.9528           0.8120            0.8394                0.9505   \n",
       "\n",
       "       Pleural Other_auc  Fracture_auc  Support Devices_auc  \n",
       "mean              0.5975        0.4756               0.7099  \n",
       "lower             0.4356        0.2319               0.6621  \n",
       "upper             0.8438        0.7944               0.7530  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display AUC with confidence intervals\n",
    "bootstrap_results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean AUC: 0.7418214285714286\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean AUC: {}\".format(np.mean(bootstrap_results[1].iloc[0, :])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(bootstrap_results[1]).to_csv('chexzero_chexpert.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs197proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6f8cabb0cdd98a043b05afebe161157b38902b348c145f12380c1bad8dc6017d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
